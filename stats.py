import numpy as np

train_err = np.array([0.15489048,0.01370801,0.00674658,0.1541823,0.01370792,0.00619489,
0.01213896,0.00506939,0.01090273,0.00523738,0.01076866,0.00570739,
0.01043956,0.00607002,0.01049975,0.00607002,0.01049975,0.00607002,
0.01049975,0.00607002,0.01049975,0.00607002,0.01049975,0.00607002,
0.01049975,0.00607002,0.01049975,0.00607002,0.01049975,0.00607002,
0.01049975,0.00607002,0.01049975,0.00607002,0.01049975,0.00607002,
0.01049975,0.00607002,0.01049975,0.00607002,0.01049975,0.00607002,
0.01049975,0.00607002,0.01049975,0.00607002,0.01049975,0.00607002,
0.01049975,0.00607002,0.01049975,0.00607002,0.01049975,0.00607002,
0.01049975,0.00607002,0.01049975,0.00607002,0.01049975,0.00607002,
0.01049975,0.00607002,0.01049975,0.00607002,0.01049975,0.00607002,
0.01049975,0.00607002,0.01049975,0.00607002,0.19438024,0.07835224,
0.05551516,0.2664253,0.05942676,0.04698616,0.21432382,0.17769559,
0.12383123,0.03653751,0.18970428,0.05895176,0.05103543,0.25355127,
0.17223129,0.1373512,0.05370614,0.02526,0.01480574,0.1682108,
0.10389893,0.06546826,0.09198632,0.19593841,0.08597172,0.185856,
0.15423323,0.13094562,0.14263538,0.0262495,0.00910813,0.1470642,
0.01789136,0.00754203,0.11712659,0.2682439,0.10704167,0.2707579,
0.04881148,0.05762608,0.08166794,0.2809955,0.07503448,0.25278217,
0.05793382,0.06680507,0.05689917,0.24214296,0.05255843,0.02401433,
0.02064978,3.210642,0.4292814,0.1355106,0.41896412,2.8633025,
0.32512233,0.14112958,0.37126616,0.09348118,0.01339081,0.0111937,
0.00895934,2.8211083,0.01912181,0.01164678,0.00957243,0.01170285,
0.01079075,0.05297137,0.00964611,0.05444665,0.50149727,0.01575229,
0.2646344,0.13508484,2.8315659,0.01475266,0.45731878,0.22494076,
2.8519201,0.06433286,0.00994448,0.01482784,0.00992942,0.01042064,
0.05468816,0.02154989,0.05455847,0.5012497,0.21753229,0.19511189,
0.39029738,0.03831794,0.03555802,0.0236392,0.01431431,0.47771218,
0.02002739,0.00877884,0.16102956,0.15950496,0.01733647,0.41761053,
0.0080731,0.15266448,0.01306833,0.00687046,0.01277428,0.00562538,
0.01121261,0.00617572,0.00892338,0.00570739,0.01043956,0.00607002,
0.4853563,0.02302098,0.41911647,0.0106162,0.01333727,0.00816012,
0.01154548,0.00667823,0.00821349,0.0073649,0.01007844,0.00516912,
0.01049975,0.00607002,0.01049975,0.00607002,0.01049975,0.00607002,
0.01049975,0.00607002,0.01049975,0.00607002,0.01049975,0.00607002,
0.01049975,0.00607002,0.4853563,0.02302098,0.01149411,0.01328909,
0.00748247,0.41577157,0.01264538,0.00716673,0.00958534,0.00661552,
0.01107125,0.00665746,0.01065645,0.00614385,0.00825856,0.00607002,
0.01049975,0.00607002,0.01049975,0.00607002,0.01049975,0.00607002,
0.01049975,0.00607002,0.01049975,0.00607002,0.01049975,0.00607002,
0.01049975,0.00607002,0.01049975,0.00607002,0.01049975,0.00607002,
0.19438024,0.07835224,0.2695836,0.05864856,0.05789979,0.04553875,
0.21650995,0.17980343,0.1212944,0.03156845,0.18076228,0.05998173,
0.04270556,0.25356334,0.1722314,0.13734451,0.05370837,0.02525552,
0.01480146,0.16821449,0.10389755,0.06546979,0.19881715,0.1052928,
0.08516023,0.18816347,0.1558838,0.13049424,0.14234628,0.02649552,
0.00925866,0.14690585,0.01812812,0.00754203,0.34704468,0.08205786,
0.06045849,0.250508,0.26246414,0.0475202,0.10636902,0.05490008,
0.2695877,0.08130725,0.03700918,0.14259166,0.24429224,0.11447759,
0.03047596,0.00672596,0.0206754,0.00820664,0.25481895,0.01221239,
0.0065407,0.0120636,0.00697086,0.47893575,0.02820792,0.01388241,
0.01444885,0.41567385,0.008842,0.01219805,0.00699354,0.00928147,
0.00672287,0.15481214,0.155079,0.01305707,0.41423252,0.00804736,
0.15440816,0.01426095,0.00687046,0.01277428,0.00562538,0.01121261,
0.00617572,0.00892338,0.00570739,0.01043956,0.00607002,0.01049975,
0.00607002,0.01049975,0.41528407,0.00782369,0.01179994,0.00704799,
0.01124864,0.00675496,0.01089529,0.00650738,0.01007844,0.00516912,
0.01049975,0.00607002,0.01049975,0.00607002,0.01049975,0.00607002,
0.01049975,0.00607002,0.01049975,0.00607002,0.01049975,0.00607002,
0.01049975,0.00607002,0.01049975,0.00607002,0.01049975,0.00607002,
0.4153178,0.01237947,0.00734467,0.01146831,0.00687535,0.01107125,
0.00665746,0.01065645,0.00614385,0.00825856,0.00607002,0.01049975,
0.00607002,0.01049975,0.00607002,0.01049975,0.00607002,0.01049975,
0.00607002,0.01049975,0.00607002,0.01049975,0.00607002,0.01049975,
0.00607002,0.01049975,0.19492479,0.07864454,0.05548394,0.2668207,
0.05944953,0.04642221,0.21406999,0.17817937,0.12323818,0.02836299,
0.02207516,0.15086462,0.06569359,0.05565341,0.19410902,0.15875413,
0.11526842,0.03577515,0.02364715,0.1627616,0.12197745,0.08092353,
0.22414847,0.11200986,0.09831137,0.17842397,0.15590422,0.13100095,
0.02969211,0.15114549,0.01896279,0.00746713,0.14969833,0.01738182,
0.00828396,0.31167138,0.0628479,0.05113856,0.250508,0.10328557,
0.26864365,0.04722809,0.0636594,0.0641055,0.26501486,0.08206552,
0.25278217,0.05793382,0.07210005,0.21795093,0.10423252,0.03024456,
0.00988498,0.01214219,0.00606427,0.01041387,0.00379908,0.01075668,
0.00441197,0.01049975,0.00607002,0.15435888,0.0119628,0.00644015,
0.15437295,0.01283255,0.00641862,0.15368971,0.01320725,0.00579695,
0.15209252,0.1542558,0.01513805,0.00577876,0.15338574,0.01415806,
0.38359147,0.4393608,0.03164032,0.02700422,0.0123054,0.01685628,
0.00849286,0.01215103,0.004176,0.46390516,0.01952086,0.01023906,
0.01405118,0.00828939,0.01261657,0.00734857,0.011449,0.00539166,
0.00929488,0.00607002,0.01049975,0.00607002,0.01049975,0.00607002,
0.01049975,0.00607002,0.01049975,0.00607002,0.3709183,0.09869652,
0.10669756,0.05169456,0.27365655,0.07476826,0.07056876,0.25450513,
0.08413941,0.05399796,0.01844674,0.01440386,0.01208948,0.01506831,
0.34649134,0.06637666,0.11786502,0.05665389,0.02256917,0.01030042,
0.01241946,0.3881999,0.08954036,0.12023702,0.2416566,0.05112899,
0.14207786,0.23682433,0.07998475,0.01988047,0.0672905,0.00879277,
0.01211565,0.16115159,0.01098193,0.30694345,0.07439799,0.24626645,
0.25395334,0.06256903,0.04503578,0.10303462,0.27982694,0.16267167,
0.22554635,0.09562287,0.14473124,0.03210221,0.28122002,0.1249136,
0.01961821,0.0397429,0.00661786,0.02404505,0.32414055,0.16897573,
0.06432367,0.07186349,0.21962023,0.08568712,0.00519445,0.01571916,
0.00767623,0.01162978,0.00616497,0.0106873,0.00382913,0.00969129,
0.00607002,0.01049975,0.19492479,0.07864454,0.26967704,0.05830003,
0.05792193,0.04499263,0.21628469,0.1802574,0.12052958,0.02460688,
0.02181408,0.14998883,0.06552067,0.2698594,0.14223506,0.15854524,
0.11220594,0.03380467,0.02335123,0.01333353,0.16989155,0.09620631,
0.06642038,0.2013538,0.1052928,0.08513426,0.18816052,0.15588106,
0.13049468,0.14234643,0.02649543,0.00925835,0.1469061,0.01812837,
0.00754174,0.0155764,0.33137825,0.07655821,0.05440296,0.07669148,
0.28003976,0.07344354,0.01005354,0.01176361,0.00823131,0.01881498,
0.00537088,0.01020864,0.00510886,0.0101724,0.00607003,0.01049975])

def reject_outliers(data, m = 3.):
  d = np.abs(data - np.median(data))
  mdev = np.median(d)
  s = d/mdev if mdev else 0.
  return data[s<m]

train_err_trim = train_err[train_err<2.5]#reject_outliers(train_err)

print(f"Mean and std before: m {np.mean(train_err)}, s {np.std(train_err)}, mean and std after: m {np.mean(train_err_trim)}, s {np.std(train_err_trim)}")
#l_mean = np.mean(train_err)
#l_std = np.std(train_err)
#l_mean = np.mean(train_err_trim)
#l_std = np.std(train_err_trim)
#print(l_mean)
#print(l_std)